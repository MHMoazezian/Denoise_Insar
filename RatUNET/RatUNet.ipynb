{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31148897-c833-4462-a459-7a27984f64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, maxabs_scale, minmax_scale, normalize\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e7f56-f95f-41e8-87fb-5cdd5933d97c",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30180ae4-e880-491f-9845-afe910740a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatUNet(nn.Module):\n",
    "    def __init__(self, block, num_features=64):\n",
    "        super(RatUNet, self).__init__()\n",
    "        self.inplanes = num_features\n",
    "        \n",
    "        self.conv = nn.Conv2d(3, num_features, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, 128, 3, stride=2)\n",
    "        self.layer2 = self._make_layer(block, 128, 256, 3, stride=2)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.layer3 = self._make_layer(block, 256, 512, 4, stride=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "\n",
    "        self.layer4 = self._make_layer(block, 256, 256, 3)\n",
    "        self.layer5 = self._make_layer(block, 128, 128, 3)\n",
    "        self.layer6 = self._make_layer(block, 128, 128, 2)\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                                   nn.PReLU(),\n",
    "                                   nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                                   nn.PReLU(),\n",
    "                                   nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                                   nn.PReLU(),\n",
    "                                   nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128, bias=True),\n",
    "                                   nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   )\n",
    "        self.ca = SequentialPolarizedSelfAttention(128)\n",
    "        self.lastconv = nn.Conv2d(128, 3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0.0, math.sqrt(1.0 / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        self.inplanes = inplanes\n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                    nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, bias=True),\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=stride),\n",
    "            )\n",
    "\n",
    "        \n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        res = self.conv(x)\n",
    "\n",
    "        res2 = self.layer1(res)\n",
    "        res3 = self.layer2(res2)        \n",
    "        out = self.layer3(res3)\n",
    "        \n",
    "        out = self.deconv1(out)               \n",
    "        out = self.layer4(out)\n",
    "        out = torch.cat((out, res3), dim=1) \n",
    "\n",
    "        out = self.deconv2(out)        \n",
    "        out = self.layer5(out)\n",
    "        out = torch.cat((out, res2), dim=1)\n",
    "        \n",
    "        out = self.deconv3(out)        \n",
    "        out = self.layer6(out)\n",
    "        out = torch.cat((out, res), dim=1)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.ca(out)\n",
    "        out = self.lastconv(out)\n",
    "        # print(x, out, x.shape, out.shape)\n",
    "        return x - out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride = stride, padding=1, bias=True)\n",
    "        self.relu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride = 1, padding=1, bias=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual      \n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "# class ChannelAttention(nn.Module):\n",
    "#     def __init__(self, in_planes, ratio=16):\n",
    "#         super(ChannelAttention, self).__init__()\n",
    "#         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "#         self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=True),\n",
    "#                                nn.ReLU(),\n",
    "#                                nn.Conv2d(in_planes // 16, in_planes, 1, bias=True))\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         avg_out = self.fc(self.avg_pool(x))\n",
    "#         max_out = self.fc(self.max_pool(x))\n",
    "#         out = avg_out + max_out\n",
    "#         return self.sigmoid(out)\n",
    "\n",
    "# class SpatialAttention(nn.Module):\n",
    "#     def __init__(self, kernel_size=7):\n",
    "#         super(SpatialAttention, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=True)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "#         max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "#         x = torch.cat([avg_out, max_out], dim=1)\n",
    "#         x = self.conv1(x)\n",
    "#         return self.sigmoid(x)\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "class SequentialPolarizedSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, channel=512):\n",
    "        super().__init__()\n",
    "        self.ch_wv=nn.Conv2d(channel,channel//2,kernel_size=(1,1))\n",
    "        self.ch_wq=nn.Conv2d(channel,1,kernel_size=(1,1))\n",
    "        self.softmax_channel=nn.Softmax(1)\n",
    "        self.softmax_spatial=nn.Softmax(-1)\n",
    "        self.ch_wz=nn.Conv2d(channel//2,channel,kernel_size=(1,1))\n",
    "        self.ln=nn.LayerNorm(channel)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.sp_wv=nn.Conv2d(channel,channel//2,kernel_size=(1,1))\n",
    "        self.sp_wq=nn.Conv2d(channel,channel//2,kernel_size=(1,1))\n",
    "        self.agp=nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        #Channel-only Self-Attention\n",
    "        channel_wv=self.ch_wv(x) #bs,c//2,h,w\n",
    "        channel_wq=self.ch_wq(x) #bs,1,h,w\n",
    "        channel_wv=channel_wv.reshape(b,c//2,-1) #bs,c//2,h*w\n",
    "        channel_wq=channel_wq.reshape(b,-1,1) #bs,h*w,1\n",
    "        channel_wq=self.softmax_channel(channel_wq)\n",
    "        channel_wz=torch.matmul(channel_wv,channel_wq).unsqueeze(-1) #bs,c//2,1,1\n",
    "        channel_weight=self.sigmoid(self.ch_wz(channel_wz).reshape(b,c,1).permute(0,2,1)).permute(0,2,1).reshape(b,c,1,1) #bs,c,1,1self.ln(\n",
    "        channel_out=channel_weight*x\n",
    "\n",
    "        #Spatial-only Self-Attention\n",
    "        spatial_wv=self.sp_wv(channel_out) #bs,c//2,h,w\n",
    "        spatial_wq=self.sp_wq(channel_out) #bs,c//2,h,w\n",
    "        spatial_wq=self.agp(spatial_wq) #bs,c//2,1,1\n",
    "        spatial_wv=spatial_wv.reshape(b,c//2,-1) #bs,c//2,h*w\n",
    "        spatial_wq=spatial_wq.permute(0,2,3,1).reshape(b,1,c//2) #bs,1,c//2\n",
    "        spatial_wq=self.softmax_spatial(spatial_wq)\n",
    "        spatial_wz=torch.matmul(spatial_wq,spatial_wv) #bs,1,h*w\n",
    "        spatial_weight=self.sigmoid(spatial_wz.reshape(b,1,h,w)) #bs,1,h,w\n",
    "        spatial_out=spatial_weight*channel_out\n",
    "        \n",
    "        return spatial_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b98f12-6f98-4410-8a1f-fa3381946b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/workspace/RatUNET/weights/model380_368.1856114207448.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0563a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb86f60-ce38-4fc7-b1d1-07e33a8bb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RatUNet(BasicBlock, 64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore if you want continue with trained model\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1])\n",
    "# model = torch.nn.DistributedDataParallel(model, device_ids=list(range(torch.cuda.device_count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64adfe6-cfc7-48a5-b549-47bed6d2ddef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b8af9-99a2-4f76-91e0-4fe2dc007a60",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2856e-97bc-4ccc-9c01-22e549dfed23",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a43697-d591-4dfb-a799-68092f50bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingDataset(Dataset):\n",
    "    def __init__(self, Dataset):\n",
    "        self.data = Dataset\n",
    "        self.x = self.data[:,:,:,:3]\n",
    "        self.y = self.data[:,:,:,3:] # we only pick one channel (3rd channel)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = 256\n",
    "        \n",
    "        # X\n",
    "        noisy = self.x[idx]\n",
    "        # noisy = noisy[:size,:size,:]\n",
    "        noisy = transforms.ToTensor()(noisy)\n",
    "        \n",
    "        \n",
    "        # Y\n",
    "        \n",
    "        # approach 1\n",
    "        lbl = self.y[idx]\n",
    "        label = np.zeros((size,size,3))\n",
    "        label[:,:,:2] = lbl[:size,:size,:]\n",
    "        # label[:,:,2] = label[:,:,0]*label[:,:,1]\n",
    "        label[:,:,2] = label[:,:,0]\n",
    "        label = transforms.ToTensor()(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # approach 2\n",
    "        # lbl = self.y[idx]\n",
    "        # label = np.zeros((size,size,3))\n",
    "        # label[:,:,:2] = lbl[:size,:size,:]\n",
    "        # label = transforms.ToTensor()(label)\n",
    "\n",
    "        \n",
    "        \n",
    "        # approach 3\n",
    "        # label = self.y[idx]\n",
    "        # label = label[:,:,0].astype('float32')\n",
    "        # label = cv2.cvtColor(label, cv2.COLOR_GRAY2RGB)\n",
    "        # label = transforms.ToTensor()(label)\n",
    "\n",
    "        \n",
    "        \n",
    "        # return (noisy,idx) , (label,idx)\n",
    "        return noisy, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/workspace/data/Final_Data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4404b4-e79a-4534-a371-aed48294ea50",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b150d4-443b-43fc-983f-b6e5023f6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9587f-27e8-468a-8bdc-009963757d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117fe91a-7d83-4448-bf64-bf15e70401e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b61d64-3b3b-401c-9993-99a75e9ac975",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af2ea3c-eda2-4302-8e0c-0dcce0374267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')#.L1Loss(reduction='sum')\n",
    "criterion.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# sgdr = CosineAnnealingLR(optimizer, 50 * len(train_set), eta_min=0.0, last_epoch=-1)\n",
    "\n",
    "for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7d70d-9217-4fab-938a-7521f7560006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_loss_list = []\n",
    "#val_loss_list = []\n",
    "    \n",
    "for epoch in range(191 , 500):\n",
    "    epoch_losses = AverageMeter()\n",
    "    \n",
    "    chunks = 10\n",
    "    chunk = int(len(data) / chunks)\n",
    "    offset = 0\n",
    "    \n",
    "    \n",
    "    # Chunk level\n",
    "    for i in range(chunks):\n",
    "    \n",
    "        chunkarry = data[offset:chunk, :, :, :]\n",
    "        \n",
    "        offset = chunk\n",
    "        chunk += int(len(data) / chunks)\n",
    "        \n",
    "        dataset = DenoisingDataset(chunkarry)\n",
    "        \n",
    "        sgdr = CosineAnnealingLR(optimizer, 50 * len(dataset), eta_min=0.0, last_epoch=-1)\n",
    "        \n",
    "        train_set_size = int(.8*len(dataset))\n",
    "        val_set_size = len(dataset)-train_set_size\n",
    "        train_data, val_data= random_split(dataset, [train_set_size, val_set_size])\n",
    "        \n",
    "        train_set = DataLoader(dataset=train_data, num_workers=0, batch_size=4, shuffle=True)\n",
    "        val_set = DataLoader(dataset=val_data, num_workers=0, batch_size=4, shuffle=True)\n",
    "    \n",
    "        # Train\n",
    "        # Batch level\n",
    "        for train, val in zip(train_set, val_set):\n",
    "            model.eval()\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            noisy = train[0].to(device=device, dtype=torch.float)\n",
    "            denoised = train[1].to(device=device, dtype=torch.float)\n",
    "\n",
    "            val_noisy = val[0].to(device=device, dtype=torch.float)\n",
    "            val_denoised = val[1].to(device=device, dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "            out_train = model(noisy)\n",
    "            out_val = model(val_noisy)\n",
    "\n",
    "            train_loss = criterion(out_train, denoised) / (noisy.size()[0]*2)\n",
    "            val_loss = criterion(out_val, val_denoised) / (val_noisy.size()[0]*2)\n",
    "            \n",
    "            #train_loss_list.append(train_loss)\n",
    "            #val_loss_list.append(val_loss)\n",
    "            \n",
    "            epoch_losses.update(train_loss.item(), len(denoised))     \n",
    "            \n",
    "            \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            sgdr.step()\n",
    "\n",
    "            \n",
    "    print(f'epoch: {epoch}', '\\n', f'Train Loss: {train_loss}', f'Average Loss: {epoch_losses.avg}', f'Val Loss: {val_loss}')\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model, os.path.join('/workspace/RatUNET/weights', f'model{epoch}_{epoch_losses.avg}.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662b343-de91-41f9-a78b-2510827388cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20a9e9-257a-4760-9e6d-9c4596836994",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchify import patchify, unpatchify\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# input image\n",
    "image = np.load('/workspace/real_data/real_data2.npy')[:,:,:]\n",
    "IMAGE = np.zeros( [ np.shape(image)[1] , np.shape(image)[2] ,np.shape(image)[0] ] )\n",
    "IMAGE[:,:,0] = image[0,:,:]\n",
    "IMAGE[:,:,1] = image[1,:,:]\n",
    "IMAGE[:,:,2] = image[2,:,:]\n",
    "\n",
    "\n",
    "# splitting the image into patches\n",
    "image_height, image_width , channel_count = IMAGE.shape\n",
    "patch_height, patch_width, step = 256, 256, 1\n",
    "patch_shape = (patch_height, patch_width, channel_count)\n",
    "patches = patchify(IMAGE, patch_shape, step=step)\n",
    "plt.imshow(patches[2,2,0,:,:,0])\n",
    "print(patches.shape)\n",
    "# output_shape = (patches.shape[0] , patches.shape[1], patches.shape[2], patches.shape[5], patches.shape[3], patches.shape[4])\n",
    "\n",
    "\n",
    "# # # processing each patch\n",
    "output_patches = np.empty(patches.shape).astype(np.float)\n",
    "print(patches.shape)\n",
    "for i in range(patches.shape[0]):\n",
    "    for j in range(patches.shape[1]):\n",
    "        patch = patches[i, j, 0]\n",
    "        scaler1 = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "        scaler2 = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "        patch[:,:,0] = scaler1.fit_transform(patch[:,:,0])\n",
    "        patch[:,:,1] = scaler2.fit_transform(patch[:,:,1])\n",
    "        patch[:,:,2] = scaler2.fit_transform(patch[:,:,2])\n",
    "        xx = transforms.ToTensor()(patch).to(device=device, dtype=torch.float)\n",
    "        X = xx.expand(1 , 3 , 256 , 256)\n",
    "        output_patch = model(X)  # process the patch\n",
    "        output_patches[i, j, 0] = output_patch.cpu().detach().numpy()[0,:,:,:].transpose(1,2,0)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010674fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_patches[2, 3, 0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dfdca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merging back patches\n",
    "output_height = image_height - (image_height - patch_height) % step\n",
    "output_width = image_width - (image_width - patch_width) % step\n",
    "output_shape = (output_height, output_width, channel_count)\n",
    "output_image = unpatchify(output_patches, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc48a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image[:,:,0].shape\n",
    "# plt.imshow(output_image[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = output_patch.cpu().detach().numpy()[0,:,:,:].transpose(1,2,0)\n",
    "plt.imshow(test[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "real_data = np.load('/workspace/real_data/real_data.npy')\n",
    "\n",
    "\n",
    "\n",
    "Data = np.zeros([256 , 256 , np.shape(real_data)[0]])\n",
    "\n",
    "\n",
    "Data[:,:,0] = patches_img1[a , b ,: ,:]\n",
    "Data[:,:,1] = patches_img2[a , b ,: ,:]\n",
    "Data[:,:,2] = patches_img3[a , b ,: ,:]\n",
    "scaler1 = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "scaler2 = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "Data[:,:,0] = scaler1.fit_transform(Data[:,:,0])\n",
    "Data[:,:,1] = scaler2.fit_transform(Data[:,:,1])\n",
    "Data[:,:,2] = scaler2.fit_transform(Data[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d572d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Data)\n",
    "plt.imshow(Data[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = transforms.ToTensor()(Data).to(device=device, dtype=torch.float)\n",
    "\n",
    "X = xx.expand(1 , 3 , 256 , 256)\n",
    "out = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753c9f8d-69c6-4db2-9754-7d9fedbefa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[5,: ,: ,:3]\n",
    "np.shape(x)\n",
    "y = data[5,:,:,3:]\n",
    "xx = transforms.ToTensor()(x).to(device=device, dtype=torch.float)\n",
    "X = xx.expand(1 , 3 , 256 , 256)\n",
    "out = model(X)\n",
    "yy = transforms.ToTensor()(y).to(device=device, dtype=torch.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_patch[0].cpu().detach().permute(1,2,0)[:,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(yy.cpu().detach().permute(1,2,0)[:,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4aa0a-8437-45d6-9242-de68abe14775",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(val_set))\n",
    "x = x.to(device=device, dtype=torch.float)\n",
    "# y = y.to(device=device, dtype=torch.float)\n",
    "out = model(x)\n",
    "\n",
    "\n",
    "channel = 0\n",
    "\n",
    "l = [e for e in x] + [e for e in y] + [e for e in out]\n",
    "\n",
    "figure = plt.figure(figsize=(13,13))\n",
    "for i in range(len(l)):\n",
    "    figure.add_subplot(3, 4, i+1)\n",
    "    plt.imshow(l[i].cpu().detach().permute(2,1,0)[:,:,channel])\n",
    "    # plt.savefig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc667008",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b161a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(out[0].permute(2,1,0).cpu().detach().numpy()*255, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21aa14e-0793-4f47-8fb0-0732002d30d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(train_set))\n",
    "print(x.shape, y.shape)\n",
    "channel = 0\n",
    "l = [e for e in x] + [e for e in y]\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "for i in range(len(l)):\n",
    "    figure.add_subplot(int(x.shape[0]), int(len(l)/x.shape[0]), i+1)\n",
    "    plt.imshow(l[i].permute(2,1,0)[:,:,channel])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47441f-5c9f-4c46-a4bd-76e67b998722",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc5047-1e7f-4ed9-92f2-4010f00aeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.cvtColor(x[0].permute(1,2,0).cpu().detach().numpy()[:,:,2]*255, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a6d8f-19ea-458f-aec8-ac5d9c3bb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf04f7b-e83b-4651-b534-bbf9f81eead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imwrite('x.png', x[0].permute(1,2,0).cpu().detach().numpy()[:,:,channel]*255)\n",
    "# cv2.imwrite('y.png', y[0].permute(1,2,0).cpu().detach().numpy()[:,:,channel]*255)\n",
    "# cv2.imwrite('out.png', out[0].permute(1,2,0).cpu().detach().numpy()[:,:,channel]*255)\n",
    "# cv2.imwrite('rgb.png', rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e73db8-e52c-4778-9239-25bb4396e28c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93ab60-e595-46b9-8514-8917e62096a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Pure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdb065-b5bf-48cf-9de7-78dcf4162a9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### X (noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0e01a-1504-40cd-9160-4bb64c5df908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_x = np.load('train_X.npy')\n",
    "data_x = np.load('train_X.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eac4a5-3cf6-4f17-888e-b7dc652dab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dae76b-d9b4-410b-a0ea-2fae05c1cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_x[0][:128,:128,:]\n",
    "xx = transforms.ToTensor()(x)\n",
    "xx = normalize(xx)\n",
    "xx = xx.to(device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4b59d-3e6d-4770-aab2-37a2d75acdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc648468-3eb9-4768-a6dd-def04f9981f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Y (denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86b8cc-ecd4-4845-a020-d14a471a0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = np.load('train_Y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c43a16-b73f-45b0-a6c9-7244b2e8085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y.shape\n",
    "x = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1f56b-1c31-49e2-8c74-a14edf51edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_y[0][:128,:128,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f8912-0ab9-48f3-8111-0a02b1bd2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(iter(y_train)).cpu().detach().numpy()[0, 1, :,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81fefb-c8c5-489e-b256-668eef9fff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea8d75-9ed8-45e9-aca4-16b4c6b0c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec4bb4-097b-44b4-b9ca-77d91b22df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = xx.expand(1,3,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac7a1b-49bb-4787-99fb-6c818c946a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152028b-992f-4656-97dd-7af49c159d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7fc9ba-400f-4a3c-b2ab-45a9c8054ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fd7ce-899f-4370-8c9f-a72a0439d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out[0][0,:,:].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb295b3-b841-436f-9bcf-0ec5f1115594",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96436d6-89b2-4e61-880c-5171fe7bdf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = data_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d181b-b116-4da8-9444-0d9c35492362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1480a-df60-47d0-a6f7-4b516f19bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350b05d-0454-4967-96be-606aa9d16bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(transforms.ToTensor()(data0[:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a91e2-47e4-4d28-9146-aca972b21e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe44143-fa65-4109-858a-6e1b91ac9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5,np.nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f16f94-4b3b-49bb-a30a-4417516a8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isnan(dataset_train[0]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e38c3-1fae-4571-8bb3-16f137451bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(a).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada1160-934c-497a-af2a-c62cf7e50e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(data_y).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b766a57-e0ec-4bee-8970-1cbd39697d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.ToTensor()(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5bc76c-a5c9-40d5-9616-a240647b803e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ed62a-93d1-41aa-81bd-afeab76b3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = np.where(data_y == 'nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816b43d-96f7-4fe7-a17f-14fcef581723",
   "metadata": {},
   "outputs": [],
   "source": [
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381cc54-243c-48f7-801f-db40d68c97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e774b-afe1-4c8c-89e9-203d7cac10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f54b8-c4d4-40c9-b758-29bf13a2eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d92a15-99ac-47af-86c6-0284ece5731e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2669b42-b58c-4a0f-b51b-95d707211727",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e053e4c-c235-4825-864c-1debb747a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e3174-c5e3-46fb-86ee-b7bcbeea7bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
